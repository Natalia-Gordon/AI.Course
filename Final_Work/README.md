# ğŸš€ Hybrid RAG System with Comprehensive Evaluation Framework

## ğŸ“‹ Project Overview

This project implements a **Hybrid RAG (Retrieval-Augmented Generation) System** specifically designed for financial document analysis, featuring a comprehensive evaluation framework using RAGAS metrics. The system combines dense (Pinecone + OpenAI embeddings) and sparse (TF-IDF) retrieval with intelligent reranking and agent-based query processing.

## ğŸ¯ Key Features

- **Hybrid Retrieval**: Combines dense and sparse retrieval methods
- **Multi-Agent Architecture**: Router, Summary, Needle, and Table QA agents
- **LlamaCloud Integration**: Structured data extraction for metadata enhancement
- **Comprehensive Evaluation**: RAGAS metrics with detailed performance analysis
- **Production-Ready Logging**: Unified logging system with component-specific tracking
- **Financial Document Focus**: Optimized for Q1 2025 financial reports and tables

## ğŸ—ï¸ System Architecture

### Core Components

```
src/
â”œâ”€â”€ core/           # Configuration and core utilities
â”œâ”€â”€ agents/         # AI agents for different query types
â”œâ”€â”€ ingest/         # Document ingestion and processing
â”œâ”€â”€ index/          # Pinecone indexing and management
â”œâ”€â”€ pipeline/       # Data processing pipeline
â”œâ”€â”€ retrieve/       # Hybrid retrieval system
â”œâ”€â”€ utils/          # Utility functions and logging
â””â”€â”€ eval/           # Comprehensive evaluation framework
```

### Agent Architecture

1. **Router Agent**: LLM-based intent classification (summary, needle, table)
2. **Summary Agent**: Generates comprehensive summaries from retrieved content
3. **Needle Agent**: Finds specific information or answers precise questions
4. **Table QA Agent**: Handles quantitative analysis and table-related queries

### Retrieval System

- **Dense Retrieval**: OpenAI embeddings + Pinecone vector database
- **Sparse Retrieval**: TF-IDF with Hebrew-aware text processing
- **Reranking**: CrossEncoder with fallback scoring
- **Hybrid Fusion**: Intelligent combination of both retrieval methods

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- OpenAI API key
- Pinecone API key
- LlamaCloud API key (optional)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd Final_Work

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys
```

### Environment Variables

```bash
OPENAI_API_KEY=your_openai_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
LLAMACLOUD_API_KEY=your_llamacloud_api_key
```

### Basic Usage

```bash
# Run the main system
python src/main.py

# Run evaluation framework
python src/eval/run_evaluation.py

# Examine Pinecone chunks
python src/eval/examine_pinecone_chunks.py
```

## ğŸ“Š Evaluation Framework

### RAGAS Metrics

The system evaluates performance using industry-standard RAGAS metrics:

- **Context Precision** â‰¥ 0.75 (Target: 133.3% âœ…)
- **Context Recall** â‰¥ 0.70 (Target: 125.0% âœ…)
- **Faithfulness** â‰¥ 0.85 (Target: 100.8% âœ…)
- **Answer Relevancy** â‰¥ 0.80 (Target: 91.8% âŒ)

### Evaluation Components

1. **Ground Truth Manager**: Manages and validates ground truth data
2. **Test Set Generator**: Creates evaluation test cases
3. **Metrics Calculator**: Calculates RAGAS metrics using LangChain evaluators
4. **RAGAS Evaluator**: Orchestrates the evaluation process
5. **Results Display**: Comprehensive results table and analysis

### Running Evaluation

```bash
# Complete evaluation pipeline
python src/eval/run_evaluation.py

# View results
python src/eval/show_results.py
```

## ğŸ”§ Configuration

### Main Configuration (`src/config.yaml`)

```yaml
# Document processing
documents_dir: data/documents
processed_dir: data/processed

# Embedding configuration
embedding:
  provider: openai
  model: text-embedding-3-small
  dim: 1536

# Pinecone configuration
pinecone:
  index_name: hybrid-rag

# Evaluation configuration
evaluation:
  ragas:
    targets:
      context_precision: 0.75
      context_recall: 0.70
      faithfulness: 0.85
      answer_relevancy: 0.80
```

### Logging Configuration

```yaml
logging:
  base_dir: logs
  levels:
    ragas_evaluation: INFO
    evaluation_runner: INFO
    ground_truth_manager: INFO
    metrics_calculator: INFO
    test_set_generator: INFO
  file_pattern: "{component}_{date}.log"
  rotation:
    max_size_mb: 10
    backup_count: 7
```

## ğŸ“ Project Structure

```
Final_Work/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                 # AI agents
â”‚   â”‚   â”œâ”€â”€ router.py          # Query routing
â”‚   â”‚   â”œâ”€â”€ summary_agent.py   # Summary generation
â”‚   â”‚   â”œâ”€â”€ needle_agent.py    # Specific information retrieval
â”‚   â”‚   â””â”€â”€ table_qa_agent.py  # Table analysis
â”‚   â”œâ”€â”€ core/                  # Core functionality
â”‚   â”‚   â”œâ”€â”€ config_manager.py  # Configuration management
â”‚   â”‚   â””â”€â”€ data_loader.py     # Data loading utilities
â”‚   â”œâ”€â”€ eval/                  # Evaluation framework
â”‚   â”‚   â”œâ”€â”€ run_evaluation.py  # Main evaluation runner
â”‚   â”‚   â”œâ”€â”€ ragas_evaluator.py # RAGAS evaluation
â”‚   â”‚   â”œâ”€â”€ metrics_calculator.py # Metrics calculation
â”‚   â”‚   â”œâ”€â”€ ground_truth_manager.py # Ground truth management
â”‚   â”‚   â”œâ”€â”€ test_set_generator.py # Test set generation
â”‚   â”‚   â”œâ”€â”€ show_results.py    # Results display
â”‚   â”‚   â”œâ”€â”€ config.py          # Evaluation configuration
â”‚   â”‚   â””â”€â”€ examine_pinecone_chunks.py # Chunk examination
â”‚   â”œâ”€â”€ ingest/                # Document ingestion
â”‚   â”œâ”€â”€ index/                 # Indexing system
â”‚   â”œâ”€â”€ pipeline/              # Data pipeline
â”‚   â”œâ”€â”€ retrieve/              # Retrieval system
â”‚   â”œâ”€â”€ utils/                 # Utilities
â”‚   â””â”€â”€ main.py                # Main application
â”œâ”€â”€ data/                      # Data files
â”‚   â”œâ”€â”€ documents/             # Source documents
â”‚   â”œâ”€â”€ processed/             # Processed data
â”‚   â””â”€â”€ eval/                  # Evaluation data
â”œâ”€â”€ logs/                      # Log files
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ§ª Testing and Validation

### Test Cases

The system includes 8 comprehensive test cases covering:
- Financial highlights and operational performance
- Revenue figures and financial metrics
- Operational improvements and business developments
- Financial performance indicators
- Business segments and performance
- Financial table analysis
- Comprehensive Q1 2025 summary
- Specific data and metrics

### Performance Metrics

- **Overall Score**: 0.867 (86.7%)
- **Targets Met**: 3/4 (75%)
- **Evaluation Strategy**: ragas_current_api
- **Total Test Cases**: 8

## ğŸ“ˆ Performance Analysis

### Current Performance

| Metric | Current | Target | Status | Performance |
|--------|---------|--------|--------|-------------|
| Context Precision | 1.000 | 0.75 | âœ… PASS | 133.3% |
| Context Recall | 0.875 | 0.70 | âœ… PASS | 125.0% |
| Faithfulness | 0.857 | 0.85 | âœ… PASS | 100.8% |
| Answer Relevancy | 0.735 | 0.80 | âŒ FAIL | 91.8% |

### Areas for Improvement

1. **Answer Relevancy**: Currently at 91.8% of target, needs improvement
2. **Test Case Optimization**: Focus on cases with lower performance
3. **Context Quality**: Maintain high context precision and recall

## ğŸ” Troubleshooting

### Common Issues

1. **API Key Errors**: Ensure all required API keys are set in `.env`
2. **Pinecone Connection**: Verify Pinecone environment and index configuration
3. **Evaluation Failures**: Check ground truth data and test set configuration
4. **Logging Issues**: Verify log directory permissions and configuration

### Debug Mode

Enable debug logging in `config.yaml`:

```yaml
logging:
  levels:
    ragas_evaluation: DEBUG
    evaluation_runner: DEBUG
```

## ğŸš€ Production Deployment

### Requirements

- **Memory**: Minimum 4GB RAM
- **Storage**: 10GB+ for documents and logs
- **API Limits**: Monitor OpenAI and Pinecone usage
- **Logging**: Configure log rotation and monitoring

### Monitoring

- **Performance Metrics**: Regular RAGAS evaluation runs
- **Log Analysis**: Monitor component performance and errors
- **API Usage**: Track OpenAI and Pinecone consumption
- **System Health**: Regular validation of ground truth and test sets

### Scaling

- **Horizontal Scaling**: Multiple instances for high availability
- **Load Balancing**: Distribute queries across instances
- **Caching**: Implement Redis for frequently accessed data
- **Async Processing**: Use Celery for background tasks

## ğŸ¤ Contributing

### Development Setup

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests and documentation
5. Submit a pull request

### Code Standards

- Follow PEP 8 style guidelines
- Add comprehensive docstrings
- Include type hints
- Write unit tests for new features

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **RAGAS**: For the evaluation framework
- **LangChain**: For the evaluation integration
- **OpenAI**: For embedding and language models
- **Pinecone**: For vector database services

## ğŸ“ Support

For questions, issues, or contributions:

1. Check the documentation
2. Review existing issues
3. Create a new issue with detailed information
4. Contact the development team

---

**Last Updated**: January 2025  
**Version**: 1.0.0  
**Status**: Production Ready âœ…
